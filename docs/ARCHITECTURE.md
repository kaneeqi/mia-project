# ðŸ—ï¸ Arquitectura del Sistema MIA

## Tabla de Contenidos
- [VisiÃ³n General](#visiÃ³n-general)
- [Componentes](#componentes)
- [Flujo de Datos](#flujo-de-datos)
- [TecnologÃ­as](#tecnologÃ­as)
- [Decisiones de DiseÃ±o](#decisiones-de-diseÃ±o)

---

## ðŸ“Š VisiÃ³n General

MIA es un sistema de mÃºltiples capas que combina:
1. **Frontend React**: Interfaz de usuario y avatar 3D
2. **Backend Node.js**: Orquestador principal
3. **MIA Service (Python)**: Modelos de IA para emociones
4. **APIs Externas**: Groq (texto), ElevenLabs (voz), Rhubarb (lip-sync)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Usuario   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Escribe mensaje
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FRONTEND (React)            â”‚
â”‚  - UI de chat                       â”‚
â”‚  - Avatar 3D (Three.js)             â”‚
â”‚  - ReproducciÃ³n de audio            â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ POST /chat
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      BACKEND (Node.js)              â”‚
â”‚  1. Recibe mensaje                  â”‚
â”‚  2. Llama a MIA Service             â”‚
â”‚  3. Genera texto (Groq)             â”‚
â”‚  4. Genera audio (ElevenLabs)       â”‚
â”‚  5. Genera lip-sync (Rhubarb)       â”‚
â”‚  6. Retorna respuesta completa      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚             â”‚
       â”‚             â”‚ POST /full_pipeline
       â”‚             â–¼
       â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚      â”‚  MIA SERVICE (Python)   â”‚
       â”‚      â”‚  - MiaMotion (BERT)     â”‚
       â”‚      â”‚  - MiaPredict (BERT)    â”‚
       â”‚      â”‚  - Mapeo a expresiones  â”‚
       â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         APIs EXTERNAS               â”‚
â”‚  - Groq (LLM)                       â”‚
â”‚  - ElevenLabs (TTS)                 â”‚
â”‚  - Rhubarb (Lip-sync)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ§© Componentes

### **1. Frontend (React + Three.js)**

**UbicaciÃ³n:** `/frontend/src/`

**Componentes Principales:**

#### `App.jsx`
- Punto de entrada de la aplicaciÃ³n
- Maneja estado global
- Renderiza Experience y UI

#### `Experience.jsx`
- Escena 3D de Three.js
- IluminaciÃ³n y cÃ¡mara
- Contenedor del Avatar

#### `Avatar.jsx`
- Carga modelo 3D (.glb)
- Maneja animaciones
- Reproduce audio y lip-sync
- Aplica expresiones faciales

```javascript
// Estructura de datos del Avatar
{
  animations: {
    Idle,
    Talking_1,
    Laughing,
    Crying,
    Angry,
    Terrified,
    Surprised
  },
  facialExpressions: {
    default,
    smile,
    sad,
    angry,
    surprised,
    funnyFace
  }
}
```

#### `UI.jsx`
- Input de texto
- BotÃ³n de enviar
- Indicador de carga

#### `useChat.jsx` (Hook)
- LÃ³gica de comunicaciÃ³n con backend
- Cola de mensajes
- Estado de reproducciÃ³n

**Flujo de renderizado:**
```
Usuario escribe â†’ useChat envÃ­a POST â†’ Backend responde â†’ 
Avatar recibe mensaje â†’ Reproduce audio â†’ Sincroniza labios â†’ 
Ejecuta animaciÃ³n â†’ Aplica expresiÃ³n facial
```

---

### **2. Backend (Node.js + Express)**

**UbicaciÃ³n:** `/backend/index.js`

**Responsabilidades:**
1. **OrquestaciÃ³n**: Coordina todos los servicios
2. **TransformaciÃ³n**: Convierte MP3 â†’ WAV para Rhubarb
3. **Limpieza**: Elimina archivos temporales
4. **Fallback**: Usa datos mock si servicios fallan

**Endpoints:**

#### `POST /chat`
```javascript
Request:
{
  "message": "Hola, Â¿cÃ³mo estÃ¡s?",
  "conversationHistory": [] // Opcional
}

Response:
{
  "messages": [
    {
      "text": "Hola, Â¿cÃ³mo estÃ¡s?",
      "audio": null,
      "lipsync": null,
      "facialExpression": "default",
      "animation": "Idle"
    },
    {
      "text": "Â¡Hola! Estoy muy bien...",
      "audio": "base64_encoded_audio",
      "lipsync": {
        "metadata": {...},
        "mouthCues": [...]
      },
      "facialExpression": "smile",
      "animation": "Talking_1",
      "emotions": {
        "user": { "emotion": "alegrÃ­a", "confidence": 0.85 },
        "agent": { "emotion": "alegrÃ­a", "confidence": 0.99 }
      }
    }
  ]
}
```

#### `GET /health`
```javascript
Response:
{
  "status": "healthy",
  "services": {
    "backend": "ok",
    "mia": { "status": "healthy", "models_loaded": true },
    "textMode": "groq",
    "elevenlabs": "configured"
  },
  "tools": {
    "ffmpeg": true,
    "rhubarb": true
  }
}
```

**Pipeline Interno:**

```javascript
// 1. Detectar emociones
const emotions = await getMiaEmotions(userMessage);

// 2. Generar texto
const textResponse = await generateTextResponse(
  userMessage, 
  emotions.agent.emotion
);

// 3. Generar audio
const audioBase64 = await generateAudio(textResponse);

// 4. Convertir MP3 â†’ WAV
const wavFile = await convertMp3ToWav(mp3File);

// 5. Generar lip-sync
const lipsyncData = await generateLipSync(audioBase64);

// 6. Retornar respuesta completa
return {
  messages: [userMsg, agentMsg]
};
```

---

### **3. MIA Service (Python + Flask)**

**UbicaciÃ³n:** `/backend/mia_service.py`

**Modelos:**

#### **MiaMotion** (DetecciÃ³n de EmociÃ³n del Usuario)
- **Arquitectura**: BERT fine-tuned
- **Input**: Texto del usuario
- **Output**: 6 emociones (alegrÃ­a, amor, tristeza, ira, miedo, sorpresa)
- **Formato**: 
  ```python
  {
    "emotion": "alegrÃ­a",
    "label": 1,
    "confidence": 0.92
  }
  ```

#### **MiaPredict** (PredicciÃ³n de Respuesta del Agente)
- **Arquitectura**: BERT fine-tuned
- **Input**: Texto del usuario
- **Output**: 2 emociones (alegrÃ­a, amor)
- **Formato**:
  ```python
  {
    "emotion": "amor",
    "label": 2,
    "confidence": 0.87,
    "facial_expression": "smile"
  }
  ```

**Endpoint Principal:**

```python
@app.route('/full_pipeline', methods=['POST'])
def full_pipeline():
    # 1. Recibir texto
    text = request.json['text']
    
    # 2. Detectar emociÃ³n usuario
    user_emotion = predict_emotion(text, mia_motion_model)
    
    # 3. Predecir respuesta agente
    agent_emotion = predict_emotion(text, mia_predict_model)
    
    # 4. Mapear a expresiÃ³n facial
    facial_expression = emotion_to_facial_expression(
        agent_emotion['emotion']
    )
    
    # 5. Retornar
    return {
        "user": user_emotion,
        "agent": {
            **agent_emotion,
            "facial_expression": facial_expression
        }
    }
```

---

### **4. APIs Externas**

#### **Groq API**
- **PropÃ³sito**: GeneraciÃ³n de texto conversacional
- **Modelo**: Llama 3.1 8B Instant
- **Latencia**: ~200-500ms
- **Rate Limit**: 30 req/min (free tier)

```javascript
const response = await axios.post(
  "https://api.groq.com/openai/v1/chat/completions",
  {
    model: "llama-3.1-8b-instant",
    messages: [
      {
        role: "system",
        content: "Eres MIA, empÃ¡tica y cÃ¡lida..."
      },
      { role: "user", content: userMessage }
    ],
    max_tokens: 100,
    temperature: 0.7
  }
);
```

#### **ElevenLabs API**
- **PropÃ³sito**: Text-to-Speech
- **Modelo**: eleven_multilingual_v2
- **Latencia**: ~1-3s dependiendo del texto
- **Rate Limit**: 10,000 caracteres/mes (free tier)

```javascript
const response = await axios.post(
  `https://api.elevenlabs.io/v1/text-to-speech/${voiceID}`,
  {
    text: text,
    model_id: "eleven_multilingual_v2",
    voice_settings: {
      stability: 0.5,
      similarity_boost: 0.75
    }
  },
  {
    responseType: "arraybuffer"
  }
);
```

#### **Rhubarb Lip Sync**
- **PropÃ³sito**: AnÃ¡lisis de fonemas para lip-sync
- **Input**: Archivo WAV (16kHz, mono)
- **Output**: JSON con visemas y timestamps
- **Latencia**: ~500ms-2s

```bash
rhubarb -f json audio.wav -o output.json
```

**Formato de salida:**
```json
{
  "metadata": {
    "soundFile": "audio.wav",
    "duration": 4.5
  },
  "mouthCues": [
    { "start": 0.0, "end": 0.2, "value": "X" },
    { "start": 0.2, "end": 0.4, "value": "B" },
    { "start": 0.4, "end": 0.6, "value": "E" }
  ]
}
```

**Visemas (8 tipos):**
- **X**: Silencio
- **A**: Abierta (ah)
- **B**: Labios cerrados (b, p, m)
- **C**: Ligeramente abierta (d, t, n)
- **D**: Dientes visibles (th)
- **E**: Sonrisa (ee)
- **F**: Labios hacia adelante (f, v)
- **G**: Garganta (k, g)
- **H**: Abierta grande (i)

---

## ðŸ”„ Flujo de Datos Completo

### **Escenario: Usuario envÃ­a "Hola, Â¿cÃ³mo estÃ¡s?"**

```
1. Frontend (UI.jsx)
   â””â”€> Usuario escribe "Hola, Â¿cÃ³mo estÃ¡s?"
   â””â”€> Click en "Enviar"
   
2. Frontend (useChat.jsx)
   â””â”€> POST http://localhost:3000/chat
       Body: { "message": "Hola, Â¿cÃ³mo estÃ¡s?" }
   
3. Backend (index.js)
   â””â”€> Recibe mensaje
   â””â”€> POST http://localhost:5000/full_pipeline
       Body: { "text": "Hola, Â¿cÃ³mo estÃ¡s?" }
   
4. MIA Service (mia_service.py)
   â””â”€> MiaMotion.predict() â†’ "alegrÃ­a" (0.85)
   â””â”€> MiaPredict.predict() â†’ "alegrÃ­a" (0.92)
   â””â”€> emotion_to_facial_expression() â†’ "smile"
   â””â”€> Return: { user: {...}, agent: {...} }
   
5. Backend (index.js)
   â””â”€> Recibe emociones
   â””â”€> POST https://api.groq.com/... 
       Prompt: "Eres MIA empÃ¡tica..." + mensaje
   â””â”€> Recibe: "Â¡Hola! Estoy muy bien, gracias..."
   
6. Backend (index.js)
   â””â”€> POST https://api.elevenlabs.io/...
       Text: "Â¡Hola! Estoy muy bien, gracias..."
   â””â”€> Recibe: Audio MP3 en base64
   
7. Backend (index.js)
   â””â”€> Guarda audio.mp3 temporalmente
   â””â”€> ffmpeg -i audio.mp3 audio.wav
   â””â”€> rhubarb -f json audio.wav -o audio.json
   â””â”€> Lee audio.json
   â””â”€> Elimina archivos temporales
   
8. Backend (index.js)
   â””â”€> Construye respuesta completa:
       {
         messages: [
           { text: "Hola...", audio: null, ... },
           { 
             text: "Â¡Hola! ...",
             audio: "base64...",
             lipsync: { mouthCues: [...] },
             facialExpression: "smile",
             animation: "Laughing"
           }
         ]
       }
   â””â”€> Return al frontend
   
9. Frontend (useChat.jsx)
   â””â”€> Recibe respuesta
   â””â”€> Agrega mensajes a la cola
   
10. Frontend (Avatar.jsx)
    â””â”€> Procesa primer mensaje (usuario) â†’ skip
    â””â”€> Procesa segundo mensaje (MIA):
        - Decodifica audio base64
        - Crea AudioBuffer
        - Aplica expresiÃ³n facial "smile"
        - Inicia animaciÃ³n "Laughing"
        - Reproduce audio
        - Sincroniza labios con mouthCues
        
11. Usuario
    â””â”€> Ve avatar hablar con labios sincronizados
    â””â”€> Escucha voz femenina cÃ¡lida
    â””â”€> Ve sonrisa y animaciÃ³n alegre
```

**Tiempo Total:** ~3-5 segundos

---

## ðŸ”§ TecnologÃ­as y JustificaciÃ³n

### **Â¿Por quÃ© React + Three.js?**
- **React**: GestiÃ³n de estado eficiente, componentes reutilizables
- **Three.js**: EstÃ¡ndar de facto para 3D en web
- **React Three Fiber**: IntegraciÃ³n declarativa de Three.js con React

### **Â¿Por quÃ© Node.js en el Backend?**
- Ecosistema maduro para APIs
- FÃ¡cil integraciÃ³n con servicios externos
- AsÃ­ncrono (ideal para mÃºltiples llamadas API)
- Compatible con FFmpeg y Rhubarb

### **Â¿Por quÃ© Python para MIA Service?**
- TensorFlow/Keras para modelos de IA
- Transformers de Hugging Face
- Ecosistema de ML mÃ¡s maduro

### **Â¿Por quÃ© Separar Backend y MIA Service?**
- **Escalabilidad**: Pueden correr en servidores diferentes
- **Mantenimiento**: CÃ³digo mÃ¡s modular
- **Performance**: Node.js para I/O, Python para ML

### **Â¿Por quÃ© BERT?**
- Pre-entrenado en espaÃ±ol
- Alta precisiÃ³n en clasificaciÃ³n de texto
- Relativamente ligero (~110M parÃ¡metros)

### **Â¿Por quÃ© Rhubarb?**
- Open source y gratuito
- Alta precisiÃ³n en lip-sync
- Multiplataforma
- No requiere GPU

---

## ðŸŽ¯ Decisiones de DiseÃ±o

### **1. Lip-sync en Backend vs Frontend**
**DecisiÃ³n:** Backend

**Razones:**
- Rhubarb es binario nativo (no disponible en browser)
- Procesamiento mÃ¡s rÃ¡pido en servidor
- No consume recursos del cliente

### **2. Audio Base64 vs Stream**
**DecisiÃ³n:** Base64

**Razones:**
- MÃ¡s simple de implementar
- Compatible con todos los navegadores
- No requiere servidor de archivos estÃ¡tico

**Trade-off:** Mayor uso de ancho de banda

### **3. Modelos Locales vs API**
**DecisiÃ³n:** HÃ­brido (BERT local, LLM en API)

**Razones:**
- Modelos pequeÃ±os (BERT) â†’ Local para baja latencia
- Modelos grandes (Llama 3.1) â†’ API por recursos

### **4. Limpieza de Archivos Temporales**
**DecisiÃ³n:** Inmediata despuÃ©s de uso

**Razones:**
- Evita acumulaciÃ³n de archivos
- Reduce uso de disco
- Mejor para privacidad

### **5. Fallback a Mock**
**DecisiÃ³n:** Siempre tener datos mock

**Razones:**
- Sistema funciona aunque servicios externos fallen
- Mejor experiencia de desarrollo
- Permite demo sin API keys

---

## ðŸ“ˆ Escalabilidad

### **Bottlenecks Actuales:**

1. **Groq API**: 30 req/min en free tier
   - **SoluciÃ³n**: Implementar rate limiting, cola de requests
   
2. **ElevenLabs**: 10k caracteres/mes gratis
   - **SoluciÃ³n**: Cachear audios comunes, plan pago
   
3. **Rhubarb**: Proceso sÃ­ncrono
   - **SoluciÃ³n**: Cola de trabajos, workers paralelos

### **Mejoras Futuras:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Load Balancerâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€> Backend Instance 1 â”€â”€> MIA Service Instance 1
       â”œâ”€> Backend Instance 2 â”€â”€> MIA Service Instance 2
       â””â”€> Backend Instance 3 â”€â”€> MIA Service Instance 3
                                  â”‚
                                  â””â”€> Redis Cache
                                  â””â”€> Message Queue
```

**Optimizaciones:**
- Redis para cachear respuestas frecuentes
- Message queue (RabbitMQ) para lip-sync asÃ­ncrono
- CDN para assets estÃ¡ticos
- WebSocket para latencia menor

---

## ðŸ” Seguridad

### **Consideraciones Actuales:**

1. **API Keys en Backend**: âœ… Correcto
   - No expuestas al frontend
   - En variables de entorno

2. **CORS**: âœ… Configurado
   - Permite solo origins especÃ­ficos en producciÃ³n

3. **Rate Limiting**: âš ï¸ Por implementar
   - Prevenir abuso de APIs

4. **Input Sanitization**: âš ï¸ Por implementar
   - Validar inputs del usuario
   - Prevenir inyecciÃ³n de cÃ³digo

### **Mejoras Recomendadas:**

```javascript
// Rate limiting
const rateLimit = require('express-rate-limit');
const limiter = rateLimit({
  windowMs: 15 * 60 * 1000, // 15 minutos
  max: 100 // 100 requests por ventana
});
app.use('/chat', limiter);

// Input validation
const { body, validationResult } = require('express-validator');
app.post('/chat', [
  body('message').isLength({ min: 1, max: 500 }).trim().escape()
], async (req, res) => {
  // ...
});
```

---

## ðŸ“š Referencias

- [React Three Fiber Docs](https://docs.pmnd.rs/react-three-fiber)
- [Three.js Manual](https://threejs.org/manual/)
- [Express Best Practices](https://expressjs.com/en/advanced/best-practice-performance.html)
- [Flask Patterns](https://flask.palletsprojects.com/en/2.3.x/patterns/)
- [BERT Paper](https://arxiv.org/abs/1810.04805)

---

**Ãšltima actualizaciÃ³n:** Noviembre 2024
